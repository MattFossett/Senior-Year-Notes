#### _9-6-2019_
<audio controls>
  <source src="/Audio/450-9-6-2019.mp3" type="audio/mpeg">
Your browser does not support the audio element. 
</audio>



## Solving Problems by Searching
### Types of agents
| reflex agent | planning agent |
| --- | --- | 
| * Consider how the world is | * Consider how the world WOULD BE |
| * Choose action based on current percept | * Decisions based on hypothesized consequences of actions | 
| * Do not consider the future consequences of actions | * Must have a model of how the world evolces in response to actions and must formulate a goal | 

### Search
* We consider the problem of designing goal-based agents    
    * Environment is fully observable, deterministic, discrete, known 
    * Agent needs to find sequence of actions that reaches the goal
    * Performance measrure defined by reaching the goal and how expensive the path is
        * Was I efficient?
    * Assume nothing is changing, nothing dynamic
#### Search Problem components
* INitial state
* Actions- up, down, left, right- regular maze 
* Transition model- 
    * If i'm in this state and perform action, where do I end up? 
    * Still in the world but new representation of the agent 
* Goal state
* Path cost- likely length moved 
* Optimal SOlution- sequence of actions that gives the lowest path cost to goal 

#### Example- Romania
* Goal is to get from one city to another
    * Successor function- Given a state and an action, returns resulting state
* initial state- Arad
* Action- go from one city to another
* Transition model- go from city A to B, you end up in B
* Goal- Bucharest
* Ends of being some of edges

### State Space Problems
* How big is the state space? 
    * Initial state, actions, and transition model define problems state space
    * Can be represented as a directed graph
#### Vacuum world again
* States- (location, status) 
* How many possible states? (Note the example from audio is done in the slides)
    * You can trace this by applying every possible action to every state
    * This transitions to different states
    * The Slides show the actual transitions 
* Actions- left, right, **suck**

#### The 8-puzzle
* Basically a sliding number puzzle
* State space is 9!/2
* Transitions are swapping tiles, 
* Each move costs one 
#### Robot motion planning
* Continuous problem since arm moves in space
* Uncertainty- not necessarily deterministic

## Search
* Need to identify these- how are things represented
    * Given:
        * Initial state
        * Actions
        * Transition model
        * Goal state
        * path cost
    * How do we find the optimal solution? 
        * Perhaps build out state space and find shortest path?
            * Won't work for larger state space, shortest path will be slow etc
### Basic Idea
* Begin at start state and expand it by making list of all possible successor states (Use a successor function!)
    * What are my possible moves from here? 
* Frontier List holds list of unexpanded states
* Try to expand as few states as possible

1. Form frontier with init state
2. Pick a state from frontier and test it for **goal**, otherwise expand it with all successors
3. Add successors to frontier   
4. Loop to 2

### Successor Function
* This is where we handle not being able to enumerate every possible states
* Given this state and action, this is how we handle getting a new one
* Given a state- apply all applicable actions and generate a list of the resulting successor states

#### Successor basic idea
* See slide example, its blindly exploring other states with no real direction. We need strategy
#### Search tree
* What if tree of sequences of actions and outcomes
* Root node corresponds to starting state
* Children of a node are generated by a successor funciton
* Only add successors if haven't been visited and not in frontier, don't add dups 

#### Outline for Search Tree
* Init the frontier using starting state
* While frontier not empty:
    * Choose a frontier node according to search strategy and take off frontier
    * Are you the goal? no- expand
#### Handling repeated states
* Every teime you expand a node, add that state to the **explored set**, do not put explored states on the frontier again

### Searching 
* Uninformed Search Problem- breadth first/depth first
* Build tree as we go

## Uninformed Search Strategies
* Search strategy is defined by picking the order of node expansion
* Uninformed: use only the info available in the problem definition
    * BFS
    * DFS
    * Iterative deepening search
    * Uniform-cost search


